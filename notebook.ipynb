{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset loading**"
      ],
      "metadata": {
        "id": "_ny530u2bi3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK04SiGsazbc",
        "outputId": "4823dd04-8307-4fad-e995-066e7eefd634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
        "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "print(df.head()) # Output of the first few rows"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preprocessing**\n",
        "\n",
        "Before being used by the model, the text goes through a few cleaning steps to make it easier to work with. Everything is turned to lowercase to keep it consistent, punctuation is removed, and common words like “the” or “is” that don’t add much meaning are taken out. Finally, the text is split into individual words (tokenized) so the model can better understand and analyze it."
      ],
      "metadata": {
        "id": "CIxAQUZkcQKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def clean(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(f'{re.escape(string.punctuation)}', '', text)\n",
        "  return text\n",
        "\n",
        "df['cleaned'] = df['message'].apply(clean)\n",
        "df['bin_label'] = df['label'].map({'ham': 0, 'spam': 1}) # Mapping labels to binary values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned'], df['bin_label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qbpyE7BgdXsT"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`test_size=0.2` ensures 20% of the data will be used for testing the dataset once trained, the remaining 80% will be used for model training. `X_train` contains training features, `X_test` contains testing features, `y_train` contains training labels, `y_test` contains testing labels."
      ],
      "metadata": {
        "id": "uJMqKepqfgAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text vectorizing**\n",
        "\n",
        "**TF-IDF** vectorizer converts text data into numerical vectors, giving higher scores to words that important in a specific message and not common across all. **TF-IDF** stands for *Term Frequency - Inverse Document Frequency* and the function is defined as $w_{x,y} = \\text{tf}(t, d) \\times \\text{idf}(t, D) = \\frac{f_{t, d}}{\\sum_{\\bar{t} \\in d} f_{\\bar t, d}} \\times \\log \\frac{N}{|\\{d \\in D : t \\in d\\}|}$\n",
        "\n"
      ],
      "metadata": {
        "id": "qDiqiMJ5oJb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2)); # Vectorizer object\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "PnpvZrBt3WUQ"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classifier training**\n",
        "\n",
        "**Multinomial Naive Bayes** is a simple yet widely used classification algorithm for text data, and it is in fact pretty naive. The model looks at how many times a certain word appears in *spam* or *ham* messages and uses that to determine wheter a message is spam or not. It uses **multinomial** **distribution** to calculate the probability of a message belonging to a certain category.\n",
        "\n",
        "Multinomial distribution is defined as $P(X)=\\frac{n!}{n_1! n_2! \\dots n_m!} p_1^{n_1}p_2^{n_2} \\dots p_m^{n^m}$, where $n$ is the number of trials, $n_i$ is the count of occurrencies for outcome $i$, $p_i$ is the probability of outcome $i$. [**Maximum Likelihood Estimation**](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation/) (MLE) is used to predict how likely each single word is *spam* or *ham*."
      ],
      "metadata": {
        "id": "71wQWoMV4XMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "model = MultinomialNB(class_prior=(0.44, 0.56))\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_vec)"
      ],
      "metadata": {
        "id": "v6_jM0iHI6Is"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model evaluation**\n",
        "\n",
        "The model is evaluated on test data using accuracy and classification report. The metrics are **precision** (how many predicted spams were actually spams), **recall** (how many real spams were caught), **F1-Score** (harmonic mean of precision and recall). A confusion matrix was added to provide a detailed breakdown of the model’s predictions, showing how many spam and non-spam messages were correctly or incorrectly classified."
      ],
      "metadata": {
        "id": "4yD06dS-JCPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nMetrics\\n____________\")\n",
        "print(\"Accuracy:\", accuracy, \"\\nReport:\\n\", report, \"\\nConfusion Matrix:\\n____________\")\n",
        "print(f'''\\t\\tCorrect\\tWrong\\n\n",
        "          Ham   {cm[0][0]}\\t{cm[0][1]}\\n\n",
        "          Spam  {cm[1][1]}\\t{cm[1][0]}\n",
        "      ''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD6I8XdjJ4Mz",
        "outputId": "c018e907-eec4-47c7-e142-175f03dabd64"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics\n",
            "____________\n",
            "Accuracy: 0.9802690582959641 \n",
            "Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       966\n",
            "           1       0.95      0.90      0.92       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.97      0.95      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            " \n",
            "Confusion Matrix:\n",
            "____________\n",
            "\t\tCorrect\tWrong\n",
            "\n",
            "          Ham   959\t7\n",
            "\n",
            "          Spam  134\t15\n",
            "      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Try a custom message**\n",
        "\n",
        "To improve the flexibility of spam detection, a custom threshold check was implemented using the `predict_thresh()` function. Unlike the default model.`predict()` method, which uses a fixed threshold of $0.5$ to classify messages, this function allows us to define our own threshold (e.g., $0.4$) for the spam probability.\n",
        "\n",
        "This is useful for fine-tuning the balance between catching more spam (recall) and avoiding false positives (precision), depending on the specific needs of the application."
      ],
      "metadata": {
        "id": "1eE9xghkKgmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_thresh(thresh, msg):\n",
        "  cleaned = clean(msg)\n",
        "  vec = vectorizer.transform([cleaned])\n",
        "  return (model.predict_proba(vec)[:, 1] >= thresh).astype(int), thresh\n",
        "\n",
        "def predict_no_thresh(msg):\n",
        "  cleaned = clean(msg)\n",
        "  vec = vectorizer.transform([cleaned])\n",
        "  return model.predict(vec)\n",
        "\n",
        "sample = str(input(\"Your message >> \"))\n",
        "\n",
        "thresh = 0.4\n",
        "prediction_thresh = predict_thresh(thresh, sample)\n",
        "prediction = predict_no_thresh(sample)\n",
        "\n",
        "print(f\"\\nPrediction (fixed probability threshold >0.5):\")\n",
        "print(\"Spam (1)\\n\" if prediction[0] == 1 else \"Ham (0)\\n\")\n",
        "print(f\"Prediction (custom probability threshold >{thresh}):\")\n",
        "print(\"Spam (1)\" if prediction_thresh[0] == 1 else \"Ham (0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW68BMeCKsX3",
        "outputId": "9086fc96-9691-4e0f-9e7d-ad3c5f74b3e6"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your message >> URGENT: Your account has been compromised. Verify now at secure-update-login.net to avoid suspension!\n",
            "\n",
            "Prediction (fixed probability threshold >0.5):\n",
            "Spam (1)\n",
            "\n",
            "Prediction (custom probability threshold >0.4):\n",
            "Spam (1)\n"
          ]
        }
      ]
    }
  ]
}